{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>stateabbr</th>\n",
       "      <th>statedesc</th>\n",
       "      <th>locationname</th>\n",
       "      <th>datasource</th>\n",
       "      <th>category</th>\n",
       "      <th>measure</th>\n",
       "      <th>data_value_unit</th>\n",
       "      <th>data_value_type</th>\n",
       "      <th>data_value</th>\n",
       "      <th>low_confidence_limit</th>\n",
       "      <th>high_confidence_limit</th>\n",
       "      <th>totalpopulation</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>locationid</th>\n",
       "      <th>categoryid</th>\n",
       "      <th>measureid</th>\n",
       "      <th>datavaluetypeid</th>\n",
       "      <th>short_question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Obesity among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>44.3</td>\n",
       "      <td>42.9</td>\n",
       "      <td>45.7</td>\n",
       "      <td>2688</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-85.25278085...</td>\n",
       "      <td>0100124</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>OBESITY</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Obesity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Andalusia</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Obesity among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>45.9</td>\n",
       "      <td>45.2</td>\n",
       "      <td>46.6</td>\n",
       "      <td>9015</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-86.47806878...</td>\n",
       "      <td>0101708</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>OBESITY</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Obesity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baileyton</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Current asthma among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>10.4</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>610</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-86.60822915...</td>\n",
       "      <td>0103676</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>CASTHMA</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Current Asthma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Beatrice</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Prevention</td>\n",
       "      <td>Cholesterol screening among adults aged &gt;=18 y...</td>\n",
       "      <td>%</td>\n",
       "      <td>Age-adjusted prevalence</td>\n",
       "      <td>87.3</td>\n",
       "      <td>85.9</td>\n",
       "      <td>88.6</td>\n",
       "      <td>301</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-87.20904641...</td>\n",
       "      <td>0104900</td>\n",
       "      <td>PREVENT</td>\n",
       "      <td>CHOLSCREEN</td>\n",
       "      <td>AgeAdjPrv</td>\n",
       "      <td>Cholesterol Screening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>AL</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Black</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Stroke among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>207</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-85.74279127...</td>\n",
       "      <td>0107120</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>STROKE</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Stroke</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year stateabbr statedesc locationname datasource         category  \\\n",
       "0  2020        AL   Alabama    Abbeville      BRFSS  Health Outcomes   \n",
       "1  2020        AL   Alabama    Andalusia      BRFSS  Health Outcomes   \n",
       "2  2020        AL   Alabama    Baileyton      BRFSS  Health Outcomes   \n",
       "3  2019        AL   Alabama     Beatrice      BRFSS       Prevention   \n",
       "4  2020        AL   Alabama        Black      BRFSS  Health Outcomes   \n",
       "\n",
       "                                             measure data_value_unit  \\\n",
       "0               Obesity among adults aged >=18 years               %   \n",
       "1               Obesity among adults aged >=18 years               %   \n",
       "2        Current asthma among adults aged >=18 years               %   \n",
       "3  Cholesterol screening among adults aged >=18 y...               %   \n",
       "4                Stroke among adults aged >=18 years               %   \n",
       "\n",
       "           data_value_type data_value low_confidence_limit  \\\n",
       "0         Crude prevalence       44.3                 42.9   \n",
       "1         Crude prevalence       45.9                 45.2   \n",
       "2         Crude prevalence       10.4                  9.9   \n",
       "3  Age-adjusted prevalence       87.3                 85.9   \n",
       "4         Crude prevalence        4.2                  3.6   \n",
       "\n",
       "  high_confidence_limit totalpopulation  \\\n",
       "0                  45.7            2688   \n",
       "1                  46.6            9015   \n",
       "2                  11.0             610   \n",
       "3                  88.6             301   \n",
       "4                   4.6             207   \n",
       "\n",
       "                                         geolocation locationid categoryid  \\\n",
       "0  {'type': 'Point', 'coordinates': [-85.25278085...    0100124    HLTHOUT   \n",
       "1  {'type': 'Point', 'coordinates': [-86.47806878...    0101708    HLTHOUT   \n",
       "2  {'type': 'Point', 'coordinates': [-86.60822915...    0103676    HLTHOUT   \n",
       "3  {'type': 'Point', 'coordinates': [-87.20904641...    0104900    PREVENT   \n",
       "4  {'type': 'Point', 'coordinates': [-85.74279127...    0107120    HLTHOUT   \n",
       "\n",
       "    measureid datavaluetypeid    short_question_text  \n",
       "0     OBESITY          CrdPrv                Obesity  \n",
       "1     OBESITY          CrdPrv                Obesity  \n",
       "2     CASTHMA          CrdPrv         Current Asthma  \n",
       "3  CHOLSCREEN       AgeAdjPrv  Cholesterol Screening  \n",
       "4      STROKE          CrdPrv                 Stroke  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Socrata(\"data.cdc.gov\", 'RR1JGCh5rZDwjeVYEvT9cDvj6')\n",
    "\n",
    "results_2022 = client.get(\"epbn-9bv3\", limit=500000)\n",
    "\n",
    "results_2022_df = pd.DataFrame.from_records(results_2022)\n",
    "results_2022_df = results_2022_df.drop(['data_value_footnote_symbol','data_value_footnote'], axis=1).dropna()\n",
    "results_2022_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                       int64\n",
       "stateabbr                 object\n",
       "statedesc                 object\n",
       "locationname              object\n",
       "datasource                object\n",
       "category                  object\n",
       "measure                   object\n",
       "data_value_unit           object\n",
       "data_value_type           object\n",
       "data_value               float64\n",
       "low_confidence_limit     float64\n",
       "high_confidence_limit    float64\n",
       "totalpopulation            int64\n",
       "geolocation               object\n",
       "locationid                object\n",
       "categoryid                object\n",
       "measureid                 object\n",
       "datavaluetypeid           object\n",
       "short_question_text       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2022_df['year'] = results_2022_df['year'].astype(int)\n",
    "results_2022_df['totalpopulation'] = results_2022_df['totalpopulation'].astype(int)\n",
    "results_2022_df['data_value'] = results_2022_df['data_value'].astype(float)\n",
    "results_2022_df['low_confidence_limit'] = results_2022_df['low_confidence_limit'].astype(float)\n",
    "results_2022_df['high_confidence_limit'] = results_2022_df['high_confidence_limit'].astype(float)\n",
    "results_2022_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>stateabbr</th>\n",
       "      <th>statedesc</th>\n",
       "      <th>locationname</th>\n",
       "      <th>datasource</th>\n",
       "      <th>category</th>\n",
       "      <th>measure</th>\n",
       "      <th>data_value_unit</th>\n",
       "      <th>data_value_type</th>\n",
       "      <th>data_value</th>\n",
       "      <th>low_confidence_limit</th>\n",
       "      <th>high_confidence_limit</th>\n",
       "      <th>totalpopulation</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>locationid</th>\n",
       "      <th>categoryid</th>\n",
       "      <th>measureid</th>\n",
       "      <th>datavaluetypeid</th>\n",
       "      <th>short_question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Kiana</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>All teeth lost among adults aged &gt;=65 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>38.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>347</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-160.4343638...</td>\n",
       "      <td>0239300</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>TEETHLOST</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>All Teeth Lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Koliganek</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Arthritis among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>25.7</td>\n",
       "      <td>209</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-157.2259091...</td>\n",
       "      <td>0241500</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>ARTHRITIS</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Kongiganak</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Arthritis among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>439</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-162.8830767...</td>\n",
       "      <td>0241610</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>ARTHRITIS</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Arthritis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Lakes</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Obesity among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>36.7</td>\n",
       "      <td>32.5</td>\n",
       "      <td>41.2</td>\n",
       "      <td>8364</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-149.3066764...</td>\n",
       "      <td>0242832</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>OBESITY</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Obesity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>AK</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>Mountain Village</td>\n",
       "      <td>BRFSS</td>\n",
       "      <td>Health Outcomes</td>\n",
       "      <td>Obesity among adults aged &gt;=18 years</td>\n",
       "      <td>%</td>\n",
       "      <td>Crude prevalence</td>\n",
       "      <td>47.3</td>\n",
       "      <td>39.2</td>\n",
       "      <td>56.1</td>\n",
       "      <td>813</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-163.7209368...</td>\n",
       "      <td>0251180</td>\n",
       "      <td>HLTHOUT</td>\n",
       "      <td>OBESITY</td>\n",
       "      <td>CrdPrv</td>\n",
       "      <td>Obesity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year stateabbr statedesc      locationname datasource         category  \\\n",
       "0  2020        AK    Alaska             Kiana      BRFSS  Health Outcomes   \n",
       "1  2021        AK    Alaska         Koliganek      BRFSS  Health Outcomes   \n",
       "2  2021        AK    Alaska        Kongiganak      BRFSS  Health Outcomes   \n",
       "3  2021        AK    Alaska             Lakes      BRFSS  Health Outcomes   \n",
       "4  2021        AK    Alaska  Mountain Village      BRFSS  Health Outcomes   \n",
       "\n",
       "                                       measure data_value_unit  \\\n",
       "0  All teeth lost among adults aged >=65 years               %   \n",
       "1       Arthritis among adults aged >=18 years               %   \n",
       "2       Arthritis among adults aged >=18 years               %   \n",
       "3         Obesity among adults aged >=18 years               %   \n",
       "4         Obesity among adults aged >=18 years               %   \n",
       "\n",
       "    data_value_type data_value low_confidence_limit high_confidence_limit  \\\n",
       "0  Crude prevalence       38.5                 29.0                  48.0   \n",
       "1  Crude prevalence       22.0                 18.5                  25.7   \n",
       "2  Crude prevalence       23.5                 20.2                  27.0   \n",
       "3  Crude prevalence       36.7                 32.5                  41.2   \n",
       "4  Crude prevalence       47.3                 39.2                  56.1   \n",
       "\n",
       "  totalpopulation                                        geolocation  \\\n",
       "0             347  {'type': 'Point', 'coordinates': [-160.4343638...   \n",
       "1             209  {'type': 'Point', 'coordinates': [-157.2259091...   \n",
       "2             439  {'type': 'Point', 'coordinates': [-162.8830767...   \n",
       "3            8364  {'type': 'Point', 'coordinates': [-149.3066764...   \n",
       "4             813  {'type': 'Point', 'coordinates': [-163.7209368...   \n",
       "\n",
       "  locationid categoryid  measureid datavaluetypeid short_question_text  \n",
       "0    0239300    HLTHOUT  TEETHLOST          CrdPrv      All Teeth Lost  \n",
       "1    0241500    HLTHOUT  ARTHRITIS          CrdPrv           Arthritis  \n",
       "2    0241610    HLTHOUT  ARTHRITIS          CrdPrv           Arthritis  \n",
       "3    0242832    HLTHOUT    OBESITY          CrdPrv             Obesity  \n",
       "4    0251180    HLTHOUT    OBESITY          CrdPrv             Obesity  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2023 = client.get(\"eav7-hnsx\", limit=500000)\n",
    "\n",
    "results_2023_df = pd.DataFrame.from_records(results_2023)\n",
    "results_2023_df = results_2023_df.drop(['data_value_footnote_symbol','data_value_footnote',':@computed_region_bxsw_vy29',':@computed_region_he4y_prf8'], axis=1).dropna()\n",
    "results_2023_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year                       int64\n",
       "stateabbr                 object\n",
       "statedesc                 object\n",
       "locationname              object\n",
       "datasource                object\n",
       "category                  object\n",
       "measure                   object\n",
       "data_value_unit           object\n",
       "data_value_type           object\n",
       "data_value               float64\n",
       "low_confidence_limit     float64\n",
       "high_confidence_limit    float64\n",
       "totalpopulation            int64\n",
       "geolocation               object\n",
       "locationid                object\n",
       "categoryid                object\n",
       "measureid                 object\n",
       "datavaluetypeid           object\n",
       "short_question_text       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2023_df['year'] = results_2023_df['year'].astype(int)\n",
    "results_2023_df['totalpopulation'] = results_2023_df['totalpopulation'].astype(int)\n",
    "results_2023_df['data_value'] = results_2023_df['data_value'].astype(float)\n",
    "results_2023_df['low_confidence_limit'] = results_2023_df['low_confidence_limit'].astype(float)\n",
    "results_2023_df['high_confidence_limit'] = results_2023_df['high_confidence_limit'].astype(float)\n",
    "results_2023_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered by Age-adjusted prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_value = ['Age-adjusted prevalence']\n",
    "age_results_2022_df = results_2022_df[results_2022_df['data_value_type'].isin(search_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_value = ['Age-adjusted prevalence']\n",
    "age_results_2023_df = results_2023_df[results_2023_df['data_value_type'].isin(search_value)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered by Health Condition: \n",
    "1. Cancer (exlcuding Skin)\n",
    "2. Coronary Heart Disease\n",
    "3. COPD\n",
    "4. Obesity\n",
    "5. Diabetes\n",
    "6. Health Insurance\n",
    "7. Annual Checkup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_1 = ['Cancer (except skin)']\n",
    "\n",
    "cancer_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_1)]\n",
    "cancer_2022_df = cancer_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "cancer_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_1)]\n",
    "cancer_2023_df = cancer_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_2 = ['Coronary Heart Disease']\n",
    "\n",
    "chd_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_2)]\n",
    "chd_2022_df = chd_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "chd_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_2)]\n",
    "chd_2023_df = chd_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_3 = ['COPD']\n",
    "\n",
    "copd_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_3)]\n",
    "copd_2022_df = copd_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "copd_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_3)]\n",
    "copd_2023_df = copd_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_4 = ['Obesity']\n",
    "\n",
    "obesity_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_4)]\n",
    "obesity_2022_df = obesity_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "obesity_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_4)]\n",
    "obesity_2023_df = obesity_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_5 = ['Diabetes']\n",
    "\n",
    "diabetes_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_5)]\n",
    "diabetes_2022_df = diabetes_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "diabetes_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_5)]\n",
    "diabates_2023_df = diabetes_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_6 = ['Health Insurance']\n",
    "\n",
    "insurance_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_6)]\n",
    "insurance_2022_df = insurance_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "insurance_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_6)]\n",
    "insurance_2023_df = insurance_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_condition_7 = ['Annual Checkup']\n",
    "\n",
    "checkup_2022_df = age_results_2022_df[age_results_2022_df['short_question_text'].isin(health_condition_7)]\n",
    "checkup_2022_df = checkup_2022_df.sort_values(['stateabbr'], ascending=True)\n",
    "\n",
    "checkup_2023_df = age_results_2023_df[age_results_2023_df['short_question_text'].isin(health_condition_7)]\n",
    "checkup_2023_df = checkup_2023_df.sort_values(['stateabbr'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered by States : Alabama, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of California, Georgia, Hawaii, Idaho, Illinois, and Indiana.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer (except Skin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_1)]\n",
    "AL_cancer_2022_df = AL_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_1)]\n",
    "AL_cancer_2023_df = AL_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_cancer_2022_df = AZ_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_cancer_2023_df = AZ_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_3)]\n",
    "AR_cancer_2022_df = AR_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_3)]\n",
    "AR_cancer_2023_df = AR_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_4)]\n",
    "CA_cancer_2022_df = CA_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_4)]\n",
    "CA_cancer_2023_df = CA_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_5)]\n",
    "CO_cancer_2022_df = CO_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_5)]\n",
    "CO_cancer_2023_df = CO_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_6)]\n",
    "CT_cancer_2022_df = CT_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_6)]\n",
    "CT_cancer_2023_df = CT_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_7)]\n",
    "DL_cancer_2022_df = DL_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_7)]\n",
    "DL_cancer_2023_df = DL_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_8)]\n",
    "DC_cancer_2022_df = DC_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_8)]\n",
    "DC_cancer_2023_df = DC_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_9)]\n",
    "GA_cancer_2022_df = GA_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_9)]\n",
    "GA_cancer_2023_df = GA_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_10)]\n",
    "HI_cancer_2022_df = HI_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_10)]\n",
    "HI_cancer_2023_df = HI_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_11)]\n",
    "ID_cancer_2022_df = ID_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_11)]\n",
    "ID_cancer_2023_df = ID_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_12)]\n",
    "IL_cancer_2022_df = IL_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_12)]\n",
    "IL_cancer_2023_df = IL_cancer_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Indiana']\n",
    "IN_cancer_2022_df = cancer_2022_df[cancer_2022_df['statedesc'].isin(state_12)]\n",
    "IN_cancer_2022_df = IN_cancer_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_cancer_2023_df = cancer_2023_df[cancer_2023_df['statedesc'].isin(state_12)]\n",
    "IN_cancer_2023_df = IN_cancer_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coronary Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_1)]\n",
    "AL_chd_2022_df = AL_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_1)]\n",
    "AL_chd_2023_df = AL_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_chd_2022_df = AZ_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_chd_2023_df = AZ_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_3)]\n",
    "AR_chd_2022_df = AR_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_3)]\n",
    "AR_chd_2023_df = AR_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_4)]\n",
    "CA_chd_2022_df = CA_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_4)]\n",
    "CA_chd_2023_df = CA_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_5)]\n",
    "CO_chd_2022_df = CO_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_5)]\n",
    "CO_chd_2023_df = CO_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_6)]\n",
    "CT_chd_2022_df = CT_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_6)]\n",
    "CT_chd_2023_df = CT_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_7)]\n",
    "DL_chd_2022_df = DL_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_7)]\n",
    "DL_chd_2023_df = DL_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_8)]\n",
    "DC_chd_2022_df = DC_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_8)]\n",
    "DC_chd_2023_df = DC_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_9)]\n",
    "GA_chd_2022_df = GA_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_9)]\n",
    "GA_chd_2023_df = GA_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_10)]\n",
    "HI_chd_2022_df = HI_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_10)]\n",
    "HI_chd_2023_df = HI_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_11)]\n",
    "ID_chd_2022_df = ID_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_11)]\n",
    "ID_chd_2023_df = ID_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_12)]\n",
    "IL_chd_2022_df = IL_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_12)]\n",
    "IL_chd_2023_df = IL_chd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_chd_2022_df = chd_2022_df[chd_2022_df['statedesc'].isin(state_13)]\n",
    "IN_chd_2022_df = IN_chd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_chd_2023_df = chd_2023_df[chd_2023_df['statedesc'].isin(state_13)]\n",
    "IN_chd_2023_df = IN_chd_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_1)]\n",
    "AL_copd_2022_df = AL_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_1)]\n",
    "AL_copd_2023_df = AL_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_copd_2022_df = AZ_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_copd_2023_df = AZ_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_3)]\n",
    "AR_copd_2022_df = AR_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_3)]\n",
    "AR_copd_2023_df = AR_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_4)]\n",
    "CA_copd_2022_df = CA_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_4)]\n",
    "CA_copd_2023_df = CA_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_5)]\n",
    "CO_copd_2022_df = CO_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_5)]\n",
    "CO_copd_2023_df = CO_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_6)]\n",
    "CT_copd_2022_df = CT_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_6)]\n",
    "CT_copd_2023_df = CT_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_7)]\n",
    "DL_copd_2022_df = DL_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_7)]\n",
    "DL_copd_2023_df = DL_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_8)]\n",
    "DC_copd_2022_df = DC_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_8)]\n",
    "DC_copd_2023_df = DC_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_9)]\n",
    "GA_copd_2022_df = GA_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_9)]\n",
    "GA_copd_2023_df = GA_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_10)]\n",
    "HI_copd_2022_df = HI_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_10)]\n",
    "HI_copd_2023_df = HI_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_11)]\n",
    "ID_copd_2022_df = ID_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_11)]\n",
    "ID_copd_2023_df = ID_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_12)]\n",
    "IL_copd_2022_df = IL_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_12)]\n",
    "IL_copd_2023_df = IL_copd_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_copd_2022_df = copd_2022_df[copd_2022_df['statedesc'].isin(state_13)]\n",
    "IN_copd_2022_df = IN_copd_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_copd_2023_df = copd_2023_df[copd_2023_df['statedesc'].isin(state_13)]\n",
    "IN_copd_2023_df = IN_copd_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obesity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_1)]\n",
    "AL_obesity_2022_df = AL_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_1)]\n",
    "AL_obesity_2023_df = AL_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_obesity_2022_df = AZ_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_obesity_2023_df = AZ_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_3)]\n",
    "AR_obesity_2022_df = AR_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_3)]\n",
    "AR_obesity_2023_df = AR_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_4)]\n",
    "CA_obesity_2022_df = CA_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_4)]\n",
    "CA_obesity_2023_df = CA_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_5)]\n",
    "CO_obesity_2022_df = CO_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_5)]\n",
    "CO_obesity_2023_df = CO_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_6)]\n",
    "CT_obesity_2022_df = CT_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_6)]\n",
    "CT_obesity_2023_df = CT_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_7)]\n",
    "DL_obesity_2022_df = DL_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_7)]\n",
    "DL_obesity_2023_df = DL_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_8)]\n",
    "DC_obesity_2022_df = DC_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_8)]\n",
    "DC_obesity_2023_df = DC_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_9)]\n",
    "GA_obesity_2022_df = GA_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_9)]\n",
    "GA_obesity_2023_df = GA_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_10)]\n",
    "HI_obesity_2022_df = HI_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_10)]\n",
    "HI_obesity_2023_df = HI_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_11)]\n",
    "ID_obesity_2022_df = ID_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_11)]\n",
    "ID_obesity_2023_df = ID_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_12)]\n",
    "IL_obesity_2022_df = IL_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_12)]\n",
    "IL_obesity_2023_df = IL_obesity_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_obesity_2022_df = obesity_2022_df[obesity_2022_df['statedesc'].isin(state_13)]\n",
    "IN_obesity_2022_df = IN_obesity_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_obesity_2023_df = obesity_2023_df[obesity_2023_df['statedesc'].isin(state_13)]\n",
    "IN_obesity_2023_df = IN_obesity_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_1)]\n",
    "AL_diabetes_2022_df = AL_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_1)]\n",
    "AL_diabetes_2023_df = AL_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_diabetes_2022_df = AZ_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_diabetes_2023_df = AZ_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_3)]\n",
    "AR_diabetes_2022_df = AR_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_3)]\n",
    "AR_diabetes_2023_df = AR_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_4)]\n",
    "CA_diabetes_2022_df = CA_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_4)]\n",
    "CA_diabetes_2023_df = CA_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_5)]\n",
    "CO_diabetes_2022_df = CO_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_5)]\n",
    "CO_diabetes_2023_df = CO_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_6)]\n",
    "CT_diabetes_2022_df = CT_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_6)]\n",
    "CT_diabetes_2023_df = CT_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_7)]\n",
    "DL_diabetes_2022_df = DL_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_7)]\n",
    "DL_diabetes_2023_df = DL_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_8)]\n",
    "DC_diabetes_2022_df = DC_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_8)]\n",
    "DC_diabetes_2023_df = DC_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_9)]\n",
    "GA_diabetes_2022_df = GA_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_9)]\n",
    "GA_diabetes_2023_df = GA_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_10)]\n",
    "HI_diabetes_2022_df = HI_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_10)]\n",
    "HI_diabetes_2023_df = HI_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_11)]\n",
    "ID_diabetes_2022_df = ID_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_11)]\n",
    "ID_diabetes_2023_df = ID_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_12)]\n",
    "IL_diabetes_2022_df = IL_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_12)]\n",
    "IL_diabetes_2023_df = IL_diabetes_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_diabetes_2022_df = diabetes_2022_df[diabetes_2022_df['statedesc'].isin(state_13)]\n",
    "IN_diabetes_2022_df = IN_diabetes_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_diabetes_2023_df = diabetes_2023_df[diabetes_2023_df['statedesc'].isin(state_13)]\n",
    "IN_diabetes_2023_df = IN_diabetes_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_1)]\n",
    "AL_insurance_2022_df = AL_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_1)]\n",
    "AL_insurance_2023_df = AL_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_insurance_2022_df = AZ_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_insurance_2023_df = AZ_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_3)]\n",
    "AR_insurance_2022_df = AR_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_3)]\n",
    "AR_insurance_2023_df = AR_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_4)]\n",
    "CA_insurance_2022_df = CA_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_4)]\n",
    "CA_insurance_2023_df = CA_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_5)]\n",
    "CO_insurance_2022_df = CO_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_5)]\n",
    "CO_insurance_2023_df = CO_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_6)]\n",
    "CT_insurance_2022_df = CT_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_6)]\n",
    "CT_insurance_2023_df = CT_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_7)]\n",
    "DL_insurance_2022_df = DL_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_7)]\n",
    "DL_insurance_2023_df = DL_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_8)]\n",
    "DC_insurance_2022_df = DC_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_8)]\n",
    "DC_insurance_2023_df = DC_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_9)]\n",
    "GA_insurance_2022_df = GA_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_9)]\n",
    "GA_insurance_2023_df = GA_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_10)]\n",
    "HI_insurance_2022_df = HI_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_10)]\n",
    "HI_insurance_2023_df = HI_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_11)]\n",
    "ID_insurance_2022_df = ID_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_11)]\n",
    "ID_insurance_2023_df = ID_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_12)]\n",
    "IL_insurance_2022_df = IL_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_12)]\n",
    "IL_insurance_2023_df = IL_insurance_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_insurance_2022_df = insurance_2022_df[insurance_2022_df['statedesc'].isin(state_13)]\n",
    "IN_insurance_2022_df = IN_insurance_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_insurance_2023_df = insurance_2023_df[insurance_2023_df['statedesc'].isin(state_13)]\n",
    "IN_insurance_2023_df = IN_insurance_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annual Checkup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_1 = ['Alabama']\n",
    "AL_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_1)]\n",
    "AL_checkup_2022_df = AL_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AL_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_1)]\n",
    "AL_checkup_2023_df = AL_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_2 = ['Arizona']\n",
    "AZ_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_2)]\n",
    "AZ_checkup_2022_df = AZ_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AZ_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_2)]\n",
    "AZ_checkup_2023_df = AZ_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_3 = ['Arkansas']\n",
    "AR_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_3)]\n",
    "AR_checkup_2022_df = AR_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "AR_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_3)]\n",
    "AR_checkup_2023_df = AR_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_4 = ['California']\n",
    "CA_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_4)]\n",
    "CA_checkup_2022_df = CA_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CA_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_4)]\n",
    "CA_checkup_2023_df = CA_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_5 = ['Colorado']\n",
    "CO_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_5)]\n",
    "CO_checkup_2022_df = CO_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CO_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_5)]\n",
    "CO_checkup_2023_df = CO_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_6 = ['Connecticut']\n",
    "CT_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_6)]\n",
    "CT_checkup_2022_df = CT_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "CT_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_6)]\n",
    "CT_checkup_2023_df = CT_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_7 = ['Delaware']\n",
    "DL_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_7)]\n",
    "DL_checkup_2022_df = DL_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DL_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_7)]\n",
    "DL_checkup_2023_df = DL_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_8 = ['District of Columbia']\n",
    "DC_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_8)]\n",
    "DC_checkup_2022_df = DC_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "DC_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_8)]\n",
    "DC_checkup_2023_df = DC_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_9 = ['Georgia']\n",
    "GA_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_9)]\n",
    "GA_checkup_2022_df = GA_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "GA_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_9)]\n",
    "GA_checkup_2023_df = GA_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_10 = ['Hawaii']\n",
    "HI_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_10)]\n",
    "HI_checkup_2022_df = HI_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "HI_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_10)]\n",
    "HI_checkup_2023_df = HI_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_11 = ['Idaho']\n",
    "ID_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_11)]\n",
    "ID_checkup_2022_df = ID_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "ID_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_11)]\n",
    "ID_checkup_2023_df = ID_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_12 = ['Illinois']\n",
    "IL_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_12)]\n",
    "IL_checkup_2022_df = IL_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IL_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_12)]\n",
    "IL_checkup_2023_df = IL_checkup_2023_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "state_13 = ['Indiana']\n",
    "IN_checkup_2022_df = checkup_2022_df[checkup_2022_df['statedesc'].isin(state_13)]\n",
    "IN_checkup_2022_df = IN_checkup_2022_df.sort_values('locationname',ascending=True)\n",
    "\n",
    "IN_checkup_2023_df = checkup_2023_df[checkup_2023_df['statedesc'].isin(state_13)]\n",
    "IN_checkup_2023_df = IN_checkup_2023_df.sort_values('locationname',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the data by state and health condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['year_y','stateabbr_y','statedesc_y', 'datasource_y', 'category_y', 'measure_y',\n",
    "       'data_value_unit_y', 'data_value_type_y','totalpopulation_y','geolocation_y', 'locationid_y', 'categoryid_y',\n",
    "       'measureid_y', 'datavaluetypeid_y','categoryid_x','measureid_x','datavaluetypeid_x','locationid_x']\n",
    "\n",
    "AL_cancer = pd.merge(AL_cancer_2022_df, AL_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_cancer = pd.merge(AZ_cancer_2022_df, AZ_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_cancer = pd.merge(AR_cancer_2022_df, AR_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_cancer = pd.merge(CA_cancer_2022_df, CA_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_cancer = pd.merge(CO_cancer_2022_df, CO_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_cancer = pd.merge(CT_cancer_2022_df, CT_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_cancer = pd.merge(DL_cancer_2022_df, DL_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_cancer = pd.merge(DC_cancer_2022_df, DC_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_cancer = pd.merge(GA_cancer_2022_df, GA_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_cancer = pd.merge(HI_cancer_2022_df, HI_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_cancer = pd.merge(ID_cancer_2022_df, ID_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_cancer = pd.merge(IL_cancer_2022_df, IL_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_cancer = pd.merge(IN_cancer_2022_df, IN_cancer_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "cancer_df = pd.concat([AL_cancer,AZ_cancer,AR_cancer,CA_cancer,CO_cancer,CT_cancer,DL_cancer,DC_cancer,GA_cancer,HI_cancer,\n",
    "                       ID_cancer,IL_cancer,IN_cancer], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coronary Heart Disease Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_chd = pd.merge(AL_chd_2022_df, AL_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_chd = pd.merge(AZ_chd_2022_df, AZ_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_chd = pd.merge(AR_chd_2022_df, AR_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_chd = pd.merge(CA_chd_2022_df, CA_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_chd = pd.merge(CO_chd_2022_df, CO_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_chd = pd.merge(CT_chd_2022_df, CT_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_chd = pd.merge(DL_chd_2022_df, DL_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_chd = pd.merge(DC_chd_2022_df, DC_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_chd = pd.merge(GA_chd_2022_df, GA_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_chd = pd.merge(HI_chd_2022_df, HI_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_chd = pd.merge(ID_chd_2022_df, ID_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_chd = pd.merge(IL_chd_2022_df, IL_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_chd = pd.merge(IN_chd_2022_df, IN_chd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "chd_df = pd.concat([AL_chd,AZ_chd,AR_chd,CA_chd,CO_chd,CT_chd,DL_chd,DC_chd,GA_chd,HI_chd,ID_chd,IL_chd,IN_chd],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPD Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_copd = pd.merge(AL_copd_2022_df, AL_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_copd = pd.merge(AZ_copd_2022_df, AZ_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_copd = pd.merge(AR_copd_2022_df, AR_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_copd = pd.merge(CA_copd_2022_df, CA_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_copd = pd.merge(CO_copd_2022_df, CO_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_copd = pd.merge(CT_copd_2022_df, CT_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_copd = pd.merge(DL_copd_2022_df, DL_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_copd = pd.merge(DC_copd_2022_df, DC_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_copd = pd.merge(GA_copd_2022_df, GA_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_copd = pd.merge(HI_copd_2022_df, HI_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_copd = pd.merge(ID_copd_2022_df, ID_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_copd = pd.merge(IL_copd_2022_df, IL_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_copd = pd.merge(IN_copd_2022_df, IN_copd_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "copd_df = pd.concat([AL_copd,AZ_copd,AR_copd,CA_copd,CO_copd,CT_copd,DL_copd,DC_copd,GA_copd,HI_copd,ID_copd,IL_copd,IN_copd],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obesity Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_obesity = pd.merge(AL_obesity_2022_df, AL_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_obesity = pd.merge(AZ_obesity_2022_df, AZ_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_obesity = pd.merge(AR_obesity_2022_df, AR_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_obesity = pd.merge(CA_obesity_2022_df, CA_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_obesity = pd.merge(CO_obesity_2022_df, CO_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_obesity = pd.merge(CT_obesity_2022_df, CT_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_obesity = pd.merge(DL_obesity_2022_df, DL_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_obesity = pd.merge(DC_obesity_2022_df, DC_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_obesity = pd.merge(GA_obesity_2022_df, GA_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_obesity = pd.merge(HI_obesity_2022_df, HI_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_obesity = pd.merge(ID_obesity_2022_df, ID_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_obesity = pd.merge(IL_obesity_2022_df, IL_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_obesity = pd.merge(IN_obesity_2022_df, IN_obesity_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "obesity_df = pd.concat([AL_obesity,AZ_obesity,AR_obesity,CA_obesity,CO_obesity,CT_obesity,DL_obesity,DC_obesity,GA_obesity,\n",
    "                       HI_obesity,ID_obesity,IL_obesity,IN_obesity],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diabetes Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_diabetes = pd.merge(AL_diabetes_2022_df, AL_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_diabetes = pd.merge(AZ_diabetes_2022_df, AZ_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_diabetes = pd.merge(AR_diabetes_2022_df, AR_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_diabetes = pd.merge(CA_diabetes_2022_df, CA_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_diabetes = pd.merge(CO_diabetes_2022_df, CO_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_diabetes = pd.merge(CT_diabetes_2022_df, CT_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_diabetes = pd.merge(DL_diabetes_2022_df, DL_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_diabetes = pd.merge(DC_diabetes_2022_df, DC_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_diabetes = pd.merge(GA_diabetes_2022_df, GA_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_diabetes = pd.merge(HI_diabetes_2022_df, HI_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_diabetes = pd.merge(ID_diabetes_2022_df, ID_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_diabetes = pd.merge(IL_diabetes_2022_df, IL_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_diabetes = pd.merge(IN_diabetes_2022_df, IN_diabetes_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "diabetes_df = pd.concat([AL_diabetes,AZ_diabetes,AR_diabetes,CA_diabetes,CO_diabetes,CT_diabetes,DL_diabetes,DC_diabetes,\n",
    "                        GA_diabetes,HI_diabetes,ID_diabetes,IL_diabetes,IN_diabetes],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health Insurance Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_insurance = pd.merge(AL_insurance_2022_df, AL_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_insurance = pd.merge(AZ_insurance_2022_df, AZ_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_insurance = pd.merge(AR_insurance_2022_df, AR_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_insurance = pd.merge(CA_insurance_2022_df, CA_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_insurance = pd.merge(CO_insurance_2022_df, CO_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_insurance = pd.merge(CT_insurance_2022_df, CT_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_insurance = pd.merge(DL_insurance_2022_df, DL_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_insurance = pd.merge(DC_insurance_2022_df, DC_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_insurance = pd.merge(GA_insurance_2022_df, GA_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_insurance = pd.merge(HI_insurance_2022_df, HI_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_insurance = pd.merge(ID_insurance_2022_df, ID_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_insurance = pd.merge(IL_insurance_2022_df, IL_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_insurance = pd.merge(IN_insurance_2022_df, IN_insurance_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "insurance_df = pd.concat([AL_insurance,AZ_insurance,AR_insurance,CA_insurance,CO_insurance,CT_insurance,DL_insurance,DC_insurance,\n",
    "                         GA_insurance,HI_insurance,ID_insurance,IL_insurance,IN_insurance],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annual Checkup Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_checkup = pd.merge(AL_checkup_2022_df, AL_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AZ_checkup = pd.merge(AZ_checkup_2022_df, AZ_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "AR_checkup = pd.merge(AR_checkup_2022_df, AR_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CA_checkup = pd.merge(CA_checkup_2022_df, CA_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CO_checkup = pd.merge(CO_checkup_2022_df, CO_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "CT_checkup = pd.merge(CT_checkup_2022_df, CT_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DL_checkup = pd.merge(DL_checkup_2022_df, DL_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "DC_checkup = pd.merge(DC_checkup_2022_df, DC_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "GA_checkup = pd.merge(GA_checkup_2022_df, GA_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "HI_checkup = pd.merge(HI_checkup_2022_df, HI_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "ID_checkup = pd.merge(ID_checkup_2022_df, ID_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IL_checkup = pd.merge(IL_checkup_2022_df, IL_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "IN_checkup = pd.merge(IN_checkup_2022_df, IN_checkup_2023_df, on='locationname', how='inner').drop(col, axis=1).dropna()\n",
    "\n",
    "checkup_df = pd.concat([AL_checkup,AZ_checkup,AR_checkup,CA_checkup,CO_checkup,CT_checkup,DL_checkup,DC_checkup,GA_checkup,\n",
    "                       HI_checkup,ID_checkup,IL_checkup,IN_checkup],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing for y (change in data value)\n",
    "\n",
    "if data_value_x < data_value_y, y = 1\n",
    "if data_value_x >= data_value_y, y = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CANCER Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_cancer.at[index,'y'] = 0\n",
    "\n",
    "AZ_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_cancer.at[index,'y'] = 0\n",
    "\n",
    "AR_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_cancer.at[index,'y'] = 0\n",
    "\n",
    "CA_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_cancer.at[index,'y'] = 0\n",
    "\n",
    "CO_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_cancer.at[index,'y'] = 0\n",
    "\n",
    "CT_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_cancer.at[index,'y'] = 0\n",
    "\n",
    "DL_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_cancer.at[index,'y'] = 0\n",
    "\n",
    "DC_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_cancer.at[index,'y'] = 0\n",
    "\n",
    "GA_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_cancer.at[index,'y'] = 0\n",
    "\n",
    "HI_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_cancer.at[index,'y'] = 0\n",
    "\n",
    "ID_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_cancer.at[index,'y'] = 0\n",
    "\n",
    "IL_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_cancer.at[index,'y'] = 0\n",
    "\n",
    "IN_cancer['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_cancer.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_cancer.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_cancer.at[index,'y'] = 0\n",
    "\n",
    "cancer_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in cancer_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        cancer_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        cancer_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORONARY HEART DISEASE Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_chd.at[index,'y'] = 0\n",
    "\n",
    "AZ_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_chd.at[index,'y'] = 0\n",
    "\n",
    "AR_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_chd.at[index,'y'] = 0\n",
    "\n",
    "CA_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_chd.at[index,'y'] = 0\n",
    "\n",
    "CO_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_chd.at[index,'y'] = 0\n",
    "\n",
    "CT_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_chd.at[index,'y'] = 0\n",
    "\n",
    "DL_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_chd.at[index,'y'] = 0\n",
    "\n",
    "DC_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_chd.at[index,'y'] = 0\n",
    "\n",
    "GA_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_chd.at[index,'y'] = 0\n",
    "\n",
    "HI_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_chd.at[index,'y'] = 0\n",
    "\n",
    "ID_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_chd.at[index,'y'] = 0\n",
    "\n",
    "IL_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_chd.at[index,'y'] = 0\n",
    "\n",
    "IN_chd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_chd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_chd.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_chd.at[index,'y'] = 0\n",
    "\n",
    "chd_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in chd_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        chd_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        chd_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPD Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_copd.at[index,'y'] = 0\n",
    "\n",
    "AZ_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_copd.at[index,'y'] = 0\n",
    "\n",
    "AR_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_copd.at[index,'y'] = 0\n",
    "\n",
    "CA_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_copd.at[index,'y'] = 0\n",
    "\n",
    "CO_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_copd.at[index,'y'] = 0\n",
    "\n",
    "CT_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_copd.at[index,'y'] = 0\n",
    "\n",
    "DL_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_copd.at[index,'y'] = 0\n",
    "\n",
    "DC_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_copd.at[index,'y'] = 0\n",
    "\n",
    "GA_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_copd.at[index,'y'] = 0\n",
    "\n",
    "HI_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_copd.at[index,'y'] = 0\n",
    "\n",
    "ID_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_copd.at[index,'y'] = 0\n",
    "\n",
    "IL_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_copd.at[index,'y'] = 0\n",
    "\n",
    "IN_copd['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_copd.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_copd.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_copd.at[index,'y'] = 0\n",
    "\n",
    "copd_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in copd_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        copd_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        copd_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBESITY Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_obesity.at[index,'y'] = 0\n",
    "\n",
    "AZ_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_obesity.at[index,'y'] = 0\n",
    "\n",
    "AR_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_obesity.at[index,'y'] = 0\n",
    "\n",
    "CA_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_obesity.at[index,'y'] = 0\n",
    "\n",
    "CO_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_obesity.at[index,'y'] = 0\n",
    "\n",
    "CT_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_obesity.at[index,'y'] = 0\n",
    "\n",
    "DL_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_obesity.at[index,'y'] = 0\n",
    "\n",
    "DC_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_obesity.at[index,'y'] = 0\n",
    "\n",
    "GA_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_obesity.at[index,'y'] = 0\n",
    "\n",
    "HI_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_obesity.at[index,'y'] = 0\n",
    "\n",
    "ID_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_obesity.at[index,'y'] = 0\n",
    "\n",
    "IL_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_obesity.at[index,'y'] = 0\n",
    "\n",
    "IN_obesity['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_obesity.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_obesity.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_obesity.at[index,'y'] = 0\n",
    "\n",
    "obesity_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in obesity_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        obesity_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        obesity_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIABATES Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_diabetes.at[index,'y'] = 0\n",
    "\n",
    "AZ_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_diabetes.at[index,'y'] = 0\n",
    "\n",
    "AR_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_diabetes.at[index,'y'] = 0\n",
    "\n",
    "CA_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_diabetes.at[index,'y'] = 0\n",
    "\n",
    "CO_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_diabetes.at[index,'y'] = 0\n",
    "\n",
    "CT_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_diabetes.at[index,'y'] = 0\n",
    "\n",
    "DL_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_diabetes.at[index,'y'] = 0\n",
    "\n",
    "DC_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_diabetes.at[index,'y'] = 0\n",
    "\n",
    "GA_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_diabetes.at[index,'y'] = 0\n",
    "\n",
    "HI_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_diabetes.at[index,'y'] = 0\n",
    "\n",
    "ID_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_diabetes.at[index,'y'] = 0\n",
    "\n",
    "IL_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_diabetes.at[index,'y'] = 0\n",
    "\n",
    "IN_diabetes['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_diabetes.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_diabetes.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_diabetes.at[index,'y'] = 0\n",
    "\n",
    "diabetes_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in diabetes_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        diabetes_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        diabetes_df.at[index,'y'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HEALTH INSURANCE Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_insurance.at[index,'y'] = 0\n",
    "\n",
    "AZ_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_insurance.at[index,'y'] = 0\n",
    "\n",
    "AR_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_insurance.at[index,'y'] = 0\n",
    "\n",
    "CA_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_insurance.at[index,'y'] = 0\n",
    "\n",
    "CO_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_insurance.at[index,'y'] = 0\n",
    "\n",
    "CT_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_insurance.at[index,'y'] = 0\n",
    "\n",
    "DL_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_insurance.at[index,'y'] = 0\n",
    "\n",
    "DC_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_insurance.at[index,'y'] = 0\n",
    "\n",
    "GA_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_insurance.at[index,'y'] = 0\n",
    "\n",
    "HI_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_insurance.at[index,'y'] = 0\n",
    "\n",
    "ID_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_insurance.at[index,'y'] = 0\n",
    "\n",
    "IL_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_insurance.at[index,'y'] = 0\n",
    "\n",
    "IN_insurance['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_insurance.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_insurance.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_insurance.at[index,'y'] = 0\n",
    "\n",
    "insurance_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in insurance_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        insurance_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        insurance_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNUAL CHECK-UP Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AL_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AL_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        AL_checkup.at[index,'y'] = 0\n",
    "\n",
    "AZ_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AZ_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AZ_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        AZ_checkup.at[index,'y'] = 0\n",
    "\n",
    "AR_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in AR_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        AR_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        AR_checkup.at[index,'y'] = 0\n",
    "\n",
    "CA_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CA_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CA_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        CA_checkup.at[index,'y'] = 0\n",
    "\n",
    "CO_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CO_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CO_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        CO_checkup.at[index,'y'] = 0\n",
    "\n",
    "CT_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in CT_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        CT_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        CT_checkup.at[index,'y'] = 0\n",
    "\n",
    "DL_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DL_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DL_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        DL_checkup.at[index,'y'] = 0\n",
    "\n",
    "DC_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in DC_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        DC_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        DC_checkup.at[index,'y'] = 0\n",
    "\n",
    "GA_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in GA_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        GA_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        GA_checkup.at[index,'y'] = 0\n",
    "\n",
    "HI_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in HI_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        HI_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        HI_checkup.at[index,'y'] = 0\n",
    "\n",
    "ID_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in ID_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        ID_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        ID_checkup.at[index,'y'] = 0\n",
    "\n",
    "IL_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IL_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IL_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        IL_checkup.at[index,'y'] = 0\n",
    "\n",
    "IN_checkup['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in IN_checkup.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        IN_checkup.at[index,'y'] = 1\n",
    "    else:\n",
    "        IN_checkup.at[index,'y'] = 0\n",
    "\n",
    "checkup_df['y'] = pd.Series(dtype=int)\n",
    "\n",
    "for index, row in checkup_df.iterrows():\n",
    "    if (row['data_value_x'] < row['data_value_y']):\n",
    "        checkup_df.at[index,'y'] = 1\n",
    "    else:\n",
    "        checkup_df.at[index,'y'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_cancer.to_json('new_databases/AL_cancer.json')\n",
    "AZ_cancer.to_json('new_databases/AZ_cancer.json')\n",
    "AR_cancer.to_json('new_databases/AR_cancer.json')\n",
    "CA_cancer.to_json('new_databases/CA_cancer.json')\n",
    "CO_cancer.to_json('new_databases/CO_cancer.json')\n",
    "CT_cancer.to_json('new_databases/CT_cancer.json')\n",
    "DL_cancer.to_json('new_databases/DL_cancer.json')\n",
    "DC_cancer.to_json('new_databases/DC_cancer.json')\n",
    "GA_cancer.to_json('new_databases/GA_cancer.json')\n",
    "HI_cancer.to_json('new_databases/HI_cancer.json')\n",
    "ID_cancer.to_json('new_databases/ID_cancer.json')\n",
    "IL_cancer.to_json('new_databases/IL_cancer.json')\n",
    "IN_cancer.to_json('new_databases/IN_cancer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_chd.to_json('new_databases/AL_chd.json')\n",
    "AZ_chd.to_json('new_databases/AZ_chd.json')\n",
    "AR_chd.to_json('new_databases/AR_chd.json')\n",
    "CA_chd.to_json('new_databases/CA_chd.json')\n",
    "CO_chd.to_json('new_databases/CO_chd.json')\n",
    "CT_chd.to_json('new_databases/CT_chd.json')\n",
    "DL_chd.to_json('new_databases/DL_chd.json')\n",
    "DC_chd.to_json('new_databases/DC_chd.json')\n",
    "GA_chd.to_json('new_databases/GA_chd.json')\n",
    "HI_chd.to_json('new_databases/HI_chd.json')\n",
    "ID_chd.to_json('new_databases/ID_chd.json')\n",
    "IL_chd.to_json('new_databases/IL_chd.json')\n",
    "IN_chd.to_json('new_databases/IN_chd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_copd.to_json('new_databases/AL_copd.json')\n",
    "AZ_copd.to_json('new_databases/AZ_copd.json')\n",
    "AR_copd.to_json('new_databases/AR_copd.json')\n",
    "CA_copd.to_json('new_databases/CA_copd.json')\n",
    "CO_copd.to_json('new_databases/CO_copd.json')\n",
    "CT_copd.to_json('new_databases/CT_copd.json')\n",
    "DL_copd.to_json('new_databases/DL_copd.json')\n",
    "DC_copd.to_json('new_databases/DC_copd.json')\n",
    "GA_copd.to_json('new_databases/GA_copd.json')\n",
    "HI_copd.to_json('new_databases/HI_copd.json')\n",
    "ID_copd.to_json('new_databases/ID_copd.json')\n",
    "IL_copd.to_json('new_databases/IL_copd.json')\n",
    "IN_copd.to_json('new_databases/IN_copd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_obesity.to_json('new_databases/AL_obesity.json')\n",
    "AZ_obesity.to_json('new_databases/AZ_obesity.json')\n",
    "AR_obesity.to_json('new_databases/AR_obesity.json')\n",
    "CA_obesity.to_json('new_databases/CA_obesity.json')\n",
    "CO_obesity.to_json('new_databases/CO_obesity.json')\n",
    "CT_obesity.to_json('new_databases/CT_obesity.json')\n",
    "DL_obesity.to_json('new_databases/DL_obesity.json')\n",
    "DC_obesity.to_json('new_databases/DC_obesity.json')\n",
    "GA_obesity.to_json('new_databases/GA_obesity.json')\n",
    "HI_obesity.to_json('new_databases/HI_obesity.json')\n",
    "ID_obesity.to_json('new_databases/ID_obesity.json')\n",
    "IL_obesity.to_json('new_databases/IL_obesity.json')\n",
    "IN_obesity.to_json('new_databases/IN_obesity.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_diabetes.to_json('new_databases/AL_diabetes.json')\n",
    "AZ_diabetes.to_json('new_databases/AZ_diabetes.json')\n",
    "AR_diabetes.to_json('new_databases/AR_diabetes.json')\n",
    "CA_diabetes.to_json('new_databases/CA_diabetes.json')\n",
    "CO_diabetes.to_json('new_databases/CO_diabetes.json')\n",
    "CT_diabetes.to_json('new_databases/CT_diabetes.json')\n",
    "DL_diabetes.to_json('new_databases/DL_diabetes.json')\n",
    "DC_diabetes.to_json('new_databases/DC_diabetes.json')\n",
    "GA_diabetes.to_json('new_databases/GA_diabetes.json')\n",
    "HI_diabetes.to_json('new_databases/HI_diabetes.json')\n",
    "ID_diabetes.to_json('new_databases/ID_diabetes.json')\n",
    "IL_diabetes.to_json('new_databases/IL_diabetes.json')\n",
    "IN_diabetes.to_json('new_databases/IN_diabetes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_insurance.to_json('new_databases/AL_insurance.json')\n",
    "AZ_insurance.to_json('new_databases/AZ_insurance.json')\n",
    "AR_insurance.to_json('new_databases/AR_insurance.json')\n",
    "CA_insurance.to_json('new_databases/CA_insurance.json')\n",
    "CO_insurance.to_json('new_databases/CO_insurance.json')\n",
    "CT_insurance.to_json('new_databases/CT_insurance.json')\n",
    "DL_insurance.to_json('new_databases/DL_insurance.json')\n",
    "DC_insurance.to_json('new_databases/DC_insurance.json')\n",
    "GA_insurance.to_json('new_databases/GA_insurance.json')\n",
    "HI_insurance.to_json('new_databases/HI_insurance.json')\n",
    "ID_insurance.to_json('new_databases/ID_insurance.json')\n",
    "IL_insurance.to_json('new_databases/IL_insurance.json')\n",
    "IN_insurance.to_json('new_databases/IN_insurance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_checkup.to_json('new_databases/AL_checkup.json')\n",
    "AZ_checkup.to_json('new_databases/AZ_checkup.json')\n",
    "AR_checkup.to_json('new_databases/AR_checkup.json')\n",
    "CA_checkup.to_json('new_databases/CA_checkup.json')\n",
    "CO_checkup.to_json('new_databases/CO_checkup.json')\n",
    "CT_checkup.to_json('new_databases/CT_checkup.json')\n",
    "DL_checkup.to_json('new_databases/DL_checkup.json')\n",
    "DC_checkup.to_json('new_databases/DC_checkup.json')\n",
    "GA_checkup.to_json('new_databases/GA_checkup.json')\n",
    "HI_checkup.to_json('new_databases/HI_checkup.json')\n",
    "ID_checkup.to_json('new_databases/ID_checkup.json')\n",
    "IL_checkup.to_json('new_databases/IL_checkup.json')\n",
    "IN_checkup.to_json('new_databases/IN_checkup.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993006993006993\n",
      "[[  0   0]\n",
      " [  1 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         1\n",
      "         1.0       0.99      1.00      1.00       142\n",
      "\n",
      "    accuracy                           0.99       143\n",
      "   macro avg       0.50      0.50      0.50       143\n",
      "weighted avg       0.99      0.99      0.99       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = AL_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00        99\n",
      "   macro avg       1.00      1.00      1.00        99\n",
      "weighted avg       1.00      1.00      1.00        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00       131\n",
      "\n",
      "    accuracy                           1.00       131\n",
      "   macro avg       1.00      1.00      1.00       131\n",
      "weighted avg       1.00      1.00      1.00       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9863013698630136\n",
      "[[  6   0]\n",
      " [  5 354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        11\n",
      "         1.0       0.99      1.00      0.99       354\n",
      "\n",
      "    accuracy                           0.99       365\n",
      "   macro avg       0.99      0.77      0.85       365\n",
      "weighted avg       0.99      0.99      0.98       365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CA_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9902912621359223\n",
      "[[16  0]\n",
      " [ 1 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97        17\n",
      "         1.0       0.99      1.00      0.99        86\n",
      "\n",
      "    accuracy                           0.99       103\n",
      "   macro avg       0.99      0.97      0.98       103\n",
      "weighted avg       0.99      0.99      0.99       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8611111111111112\n",
      "[[ 4  3]\n",
      " [ 2 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.67      0.62         6\n",
      "         1.0       0.93      0.90      0.92        30\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.75      0.78      0.77        36\n",
      "weighted avg       0.87      0.86      0.87        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb Cell 80\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m DC_cancer[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gang7777/Desktop/DataClass/American_Health_Dashboard/Filtering.ipynb#Y142sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9935483870967742\n",
      "[[  0   0]\n",
      " [  1 154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         1\n",
      "         1.0       0.99      1.00      1.00       154\n",
      "\n",
      "    accuracy                           0.99       155\n",
      "   macro avg       0.50      0.50      0.50       155\n",
      "weighted avg       0.99      0.99      0.99       155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = GA_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00        52\n",
      "   macro avg       1.00      1.00      1.00        52\n",
      "weighted avg       1.00      1.00      1.00        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.991044776119403\n",
      "[[  6   1]\n",
      " [  2 326]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.75      0.80         8\n",
      "         1.0       0.99      1.00      1.00       327\n",
      "\n",
      "    accuracy                           0.99       335\n",
      "   macro avg       0.93      0.87      0.90       335\n",
      "weighted avg       0.99      0.99      0.99       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        66\n",
      "\n",
      "    accuracy                           1.00        66\n",
      "   macro avg       1.00      1.00      1.00        66\n",
      "weighted avg       1.00      1.00      1.00        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_cancer.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_cancer['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876462938881665\n",
      "[[   1    0]\n",
      " [  19 1518]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.10        20\n",
      "         1.0       0.99      1.00      0.99      1518\n",
      "\n",
      "    accuracy                           0.99      1538\n",
      "   macro avg       0.99      0.53      0.54      1538\n",
      "weighted avg       0.99      0.99      0.98      1538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = cancer_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = cancer_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coronary Heart Disease Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       143\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AL_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "[[87  7]\n",
      " [ 2  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95        89\n",
      "         1.0       0.60      0.30      0.40        10\n",
      "\n",
      "    accuracy                           0.91        99\n",
      "   macro avg       0.76      0.64      0.68        99\n",
      "weighted avg       0.89      0.91      0.90        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9847328244274809\n",
      "[[129   2]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       129\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.98       131\n",
      "   macro avg       0.49      0.50      0.50       131\n",
      "weighted avg       0.97      0.98      0.98       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = AR_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[364   0]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       364\n",
      "         1.0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00       365\n",
      "   macro avg       1.00      1.00      1.00       365\n",
      "weighted avg       1.00      1.00      1.00       365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CA_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9805825242718447\n",
      "[[101   2]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       101\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.98       103\n",
      "   macro avg       0.49      0.50      0.50       103\n",
      "weighted avg       0.96      0.98      0.97       103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = CO_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        36\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[490], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_chd[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9935483870967742\n",
      "[[154   1]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       154\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       155\n",
      "   macro avg       0.50      0.50      0.50       155\n",
      "weighted avg       0.99      0.99      0.99       155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = GA_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00        52\n",
      "   macro avg       1.00      1.00      1.00        52\n",
      "weighted avg       1.00      1.00      1.00        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[335]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       335\n",
      "\n",
      "    accuracy                           1.00       335\n",
      "   macro avg       1.00      1.00      1.00       335\n",
      "weighted avg       1.00      1.00      1.00       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_chd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_chd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8679245283018868\n",
      "[[   9   40]\n",
      " [ 163 1325]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.18      0.05      0.08       172\n",
      "         1.0       0.89      0.97      0.93      1365\n",
      "\n",
      "    accuracy                           0.87      1537\n",
      "   macro avg       0.54      0.51      0.51      1537\n",
      "weighted avg       0.81      0.87      0.83      1537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = chd_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = chd_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COPD Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986013986013986\n",
      "[[141   2]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       141\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       143\n",
      "   macro avg       0.49      0.50      0.50       143\n",
      "weighted avg       0.97      0.99      0.98       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = AL_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n",
      "[[76  8]\n",
      " [ 1 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.99      0.94        77\n",
      "         1.0       0.93      0.64      0.76        22\n",
      "\n",
      "    accuracy                           0.91        99\n",
      "   macro avg       0.92      0.81      0.85        99\n",
      "weighted avg       0.91      0.91      0.90        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8854961832061069\n",
      "[[95 14]\n",
      " [ 1 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.99      0.93        96\n",
      "         1.0       0.95      0.60      0.74        35\n",
      "\n",
      "    accuracy                           0.89       131\n",
      "   macro avg       0.91      0.79      0.83       131\n",
      "weighted avg       0.89      0.89      0.88       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9945205479452055\n",
      "[[363   2]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       363\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       365\n",
      "   macro avg       0.50      0.50      0.50       365\n",
      "weighted avg       0.99      0.99      0.99       365\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = CA_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8932038834951457\n",
      "[[ 3  0]\n",
      " [11 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.21      0.35        14\n",
      "         1.0       0.89      1.00      0.94        89\n",
      "\n",
      "    accuracy                           0.89       103\n",
      "   macro avg       0.95      0.61      0.65       103\n",
      "weighted avg       0.90      0.89      0.86       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055555555555556\n",
      "[[23  4]\n",
      " [ 3  6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.88      0.87        26\n",
      "         1.0       0.67      0.60      0.63        10\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.76      0.74      0.75        36\n",
      "weighted avg       0.80      0.81      0.80        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[476], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_copd[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9096774193548387\n",
      "[[139  11]\n",
      " [  3   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.98      0.95       142\n",
      "         1.0       0.40      0.15      0.22        13\n",
      "\n",
      "    accuracy                           0.91       155\n",
      "   macro avg       0.66      0.57      0.59       155\n",
      "weighted avg       0.88      0.91      0.89       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = GA_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n",
      "[[18  7]\n",
      " [ 3 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.86      0.78        21\n",
      "         1.0       0.77      0.59      0.67        17\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.74      0.72      0.72        38\n",
      "weighted avg       0.74      0.74      0.73        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9615384615384616\n",
      "[[ 0  0]\n",
      " [ 2 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         2\n",
      "         1.0       0.96      1.00      0.98        50\n",
      "\n",
      "    accuracy                           0.96        52\n",
      "   macro avg       0.48      0.50      0.49        52\n",
      "weighted avg       0.92      0.96      0.94        52\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = ID_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9671641791044776\n",
      "[[  1   2]\n",
      " [  9 323]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.10      0.15        10\n",
      "         1.0       0.97      0.99      0.98       325\n",
      "\n",
      "    accuracy                           0.97       335\n",
      "   macro avg       0.65      0.55      0.57       335\n",
      "weighted avg       0.95      0.97      0.96       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.890625\n",
      "[[ 3  2]\n",
      " [ 5 54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.38      0.46         8\n",
      "         1.0       0.92      0.96      0.94        56\n",
      "\n",
      "    accuracy                           0.89        64\n",
      "   macro avg       0.76      0.67      0.70        64\n",
      "weighted avg       0.88      0.89      0.88        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_copd.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_copd['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8679245283018868\n",
      "[[   9   40]\n",
      " [ 163 1325]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.18      0.05      0.08       172\n",
      "         1.0       0.89      0.97      0.93      1365\n",
      "\n",
      "    accuracy                           0.87      1537\n",
      "   macro avg       0.54      0.51      0.51      1537\n",
      "weighted avg       0.81      0.87      0.83      1537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = copd_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = copd_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obesity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790209790209791\n",
      "[[51  1]\n",
      " [ 2 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97        53\n",
      "         1.0       0.98      0.99      0.98        90\n",
      "\n",
      "    accuracy                           0.98       143\n",
      "   macro avg       0.98      0.98      0.98       143\n",
      "weighted avg       0.98      0.98      0.98       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AL_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n",
      "[[11  1]\n",
      " [ 2 85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.85      0.88        13\n",
      "         1.0       0.98      0.99      0.98        86\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.95      0.92      0.93        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618320610687023\n",
      "[[44  1]\n",
      " [ 4 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95        48\n",
      "         1.0       0.95      0.99      0.97        83\n",
      "\n",
      "    accuracy                           0.96       131\n",
      "   macro avg       0.97      0.95      0.96       131\n",
      "weighted avg       0.96      0.96      0.96       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618320610687023\n",
      "[[44  1]\n",
      " [ 4 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95        48\n",
      "         1.0       0.95      0.99      0.97        83\n",
      "\n",
      "    accuracy                           0.96       131\n",
      "   macro avg       0.97      0.95      0.96       131\n",
      "weighted avg       0.96      0.96      0.96       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424657534246575\n",
      "[[192  13]\n",
      " [  8 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95       200\n",
      "         1.0       0.95      0.92      0.94       165\n",
      "\n",
      "    accuracy                           0.94       365\n",
      "   macro avg       0.94      0.94      0.94       365\n",
      "weighted avg       0.94      0.94      0.94       365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CA_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9611650485436893\n",
      "[[24  1]\n",
      " [ 3 75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.89      0.92        27\n",
      "         1.0       0.96      0.99      0.97        76\n",
      "\n",
      "    accuracy                           0.96       103\n",
      "   macro avg       0.96      0.94      0.95       103\n",
      "weighted avg       0.96      0.96      0.96       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[ 7  0]\n",
      " [ 0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[463], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_obesity[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354838709677419\n",
      "[[64  2]\n",
      " [ 8 81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.89      0.93        72\n",
      "         1.0       0.91      0.98      0.94        83\n",
      "\n",
      "    accuracy                           0.94       155\n",
      "   macro avg       0.94      0.93      0.93       155\n",
      "weighted avg       0.94      0.94      0.94       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = GA_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846153846153846\n",
      "[[23  6]\n",
      " [ 0 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      1.00      0.88        23\n",
      "         1.0       1.00      0.79      0.88        29\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.90      0.90      0.88        52\n",
      "weighted avg       0.91      0.88      0.88        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diabetes Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880597014925373\n",
      "[[ 62   1]\n",
      " [  3 269]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.95      0.97        65\n",
      "         1.0       0.99      1.00      0.99       270\n",
      "\n",
      "    accuracy                           0.99       335\n",
      "   macro avg       0.99      0.98      0.98       335\n",
      "weighted avg       0.99      0.99      0.99       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9682539682539683\n",
      "[[34  2]\n",
      " [ 0 27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97        34\n",
      "         1.0       1.00      0.93      0.96        29\n",
      "\n",
      "    accuracy                           0.97        63\n",
      "   macro avg       0.97      0.97      0.97        63\n",
      "weighted avg       0.97      0.97      0.97        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_obesity.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_obesity['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6197916666666666\n",
      "[[153 236]\n",
      " [348 799]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.31      0.34       501\n",
      "         1.0       0.70      0.77      0.73      1035\n",
      "\n",
      "    accuracy                           0.62      1536\n",
      "   macro avg       0.54      0.54      0.54      1536\n",
      "weighted avg       0.60      0.62      0.61      1536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = obesity_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = obesity_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diabetes Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881118881118881\n",
      "[[50  2]\n",
      " [14 77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.78      0.86        64\n",
      "         1.0       0.85      0.97      0.91        79\n",
      "\n",
      "    accuracy                           0.89       143\n",
      "   macro avg       0.90      0.88      0.88       143\n",
      "weighted avg       0.90      0.89      0.89       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AL_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8383838383838383\n",
      "[[68  6]\n",
      " [10 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.87      0.89        78\n",
      "         1.0       0.60      0.71      0.65        21\n",
      "\n",
      "    accuracy                           0.84        99\n",
      "   macro avg       0.76      0.79      0.77        99\n",
      "weighted avg       0.85      0.84      0.84        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625954198473282\n",
      "[[74  8]\n",
      " [10 39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.88      0.89        84\n",
      "         1.0       0.80      0.83      0.81        47\n",
      "\n",
      "    accuracy                           0.86       131\n",
      "   macro avg       0.85      0.86      0.85       131\n",
      "weighted avg       0.86      0.86      0.86       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9698630136986301\n",
      "[[ 25   1]\n",
      " [ 10 329]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.71      0.82        35\n",
      "         1.0       0.97      1.00      0.98       330\n",
      "\n",
      "    accuracy                           0.97       365\n",
      "   macro avg       0.97      0.86      0.90       365\n",
      "weighted avg       0.97      0.97      0.97       365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CA_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912621359223301\n",
      "[[82  8]\n",
      " [ 1 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.99      0.95        83\n",
      "         1.0       0.92      0.60      0.73        20\n",
      "\n",
      "    accuracy                           0.91       103\n",
      "   macro avg       0.92      0.79      0.84       103\n",
      "weighted avg       0.91      0.91      0.91       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n",
      "[[ 2  0]\n",
      " [ 2 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.50      0.67         4\n",
      "         1.0       0.94      1.00      0.97        32\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.97      0.75      0.82        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "[[9 0]\n",
      " [3 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.75      0.86        12\n",
      "         1.0       0.70      1.00      0.82         7\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.85      0.88      0.84        19\n",
      "weighted avg       0.89      0.84      0.84        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[447], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_diabetes[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896774193548387\n",
      "[[66  9]\n",
      " [ 7 73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89        73\n",
      "         1.0       0.91      0.89      0.90        82\n",
      "\n",
      "    accuracy                           0.90       155\n",
      "   macro avg       0.90      0.90      0.90       155\n",
      "weighted avg       0.90      0.90      0.90       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = GA_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n",
      "[[29  1]\n",
      " [ 0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98        29\n",
      "         1.0       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.94      0.96        38\n",
      "weighted avg       0.97      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653846153846154\n",
      "[[ 5  2]\n",
      " [ 5 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.50      0.59        10\n",
      "         1.0       0.89      0.95      0.92        42\n",
      "\n",
      "    accuracy                           0.87        52\n",
      "   macro avg       0.80      0.73      0.75        52\n",
      "weighted avg       0.86      0.87      0.86        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761194029850746\n",
      "[[ 23   2]\n",
      " [  6 304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.79      0.85        29\n",
      "         1.0       0.98      0.99      0.99       306\n",
      "\n",
      "    accuracy                           0.98       335\n",
      "   macro avg       0.95      0.89      0.92       335\n",
      "weighted avg       0.98      0.98      0.98       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90625\n",
      "[[11  3]\n",
      " [ 3 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.79      0.79        14\n",
      "         1.0       0.94      0.94      0.94        50\n",
      "\n",
      "    accuracy                           0.91        64\n",
      "   macro avg       0.86      0.86      0.86        64\n",
      "weighted avg       0.91      0.91      0.91        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_diabetes.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_diabetes['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8165256994144438\n",
      "[[  19   69]\n",
      " [ 213 1236]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.08      0.12       232\n",
      "         1.0       0.85      0.95      0.90      1305\n",
      "\n",
      "    accuracy                           0.82      1537\n",
      "   macro avg       0.53      0.51      0.51      1537\n",
      "weighted avg       0.76      0.82      0.78      1537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = diabetes_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = diabetes_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health Insurance Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       143\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AL_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595959595959596\n",
      "[[93  3]\n",
      " [ 1  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98        94\n",
      "         1.0       0.67      0.40      0.50         5\n",
      "\n",
      "    accuracy                           0.96        99\n",
      "   macro avg       0.82      0.69      0.74        99\n",
      "weighted avg       0.95      0.96      0.95        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9923664122137404\n",
      "[[130   1]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       130\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       131\n",
      "   macro avg       0.50      0.50      0.50       131\n",
      "weighted avg       0.98      0.99      0.99       131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = AR_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917808219178083\n",
      "[[362   3]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       362\n",
      "         1.0       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.99       365\n",
      "   macro avg       0.50      0.50      0.50       365\n",
      "weighted avg       0.98      0.99      0.99       365\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = CA_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9805825242718447\n",
      "[[101   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       102\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98       103\n",
      "   macro avg       0.50      0.50      0.50       103\n",
      "weighted avg       0.98      0.98      0.98       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        36\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[434], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_insurance[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9935483870967742\n",
      "[[154   1]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       154\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       155\n",
      "   macro avg       0.50      0.50      0.50       155\n",
      "weighted avg       0.99      0.99      0.99       155\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = GA_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        52\n",
      "\n",
      "    accuracy                           1.00        52\n",
      "   macro avg       1.00      1.00      1.00        52\n",
      "weighted avg       1.00      1.00      1.00        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[335]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       335\n",
      "\n",
      "    accuracy                           1.00       335\n",
      "   macro avg       1.00      1.00      1.00       335\n",
      "weighted avg       1.00      1.00      1.00       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_insurance.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_insurance['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993489583333334\n",
      "[[1535    1]\n",
      " [   0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      1535\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00      1536\n",
      "   macro avg       0.50      0.50      0.50      1536\n",
      "weighted avg       1.00      1.00      1.00      1536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gang7777/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = insurance_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = insurance_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annual Check-up Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440559440559441\n",
      "[[62  7]\n",
      " [ 1 73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94        63\n",
      "         1.0       0.99      0.91      0.95        80\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.95      0.94       143\n",
      "weighted avg       0.95      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AL_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AL_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98989898989899\n",
      "[[91  0]\n",
      " [ 1  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99        92\n",
      "         1.0       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.99        99\n",
      "   macro avg       0.94      0.99      0.96        99\n",
      "weighted avg       0.99      0.99      0.99        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AZ_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AZ_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9465648854961832\n",
      "[[64  5]\n",
      " [ 2 60]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95        66\n",
      "         1.0       0.97      0.92      0.94        65\n",
      "\n",
      "    accuracy                           0.95       131\n",
      "   macro avg       0.95      0.95      0.95       131\n",
      "weighted avg       0.95      0.95      0.95       131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = AR_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = AR_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917808219178083\n",
      "[[359   2]\n",
      " [  1   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       360\n",
      "         1.0       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.99       365\n",
      "   macro avg       0.87      0.80      0.83       365\n",
      "weighted avg       0.99      0.99      0.99       365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CA_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CA_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9805825242718447\n",
      "[[64  0]\n",
      " [ 2 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98        66\n",
      "         1.0       0.95      1.00      0.97        37\n",
      "\n",
      "    accuracy                           0.98       103\n",
      "   macro avg       0.97      0.98      0.98       103\n",
      "weighted avg       0.98      0.98      0.98       103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CO_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CO_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecticut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[24  0]\n",
      " [ 0 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        24\n",
      "         1.0       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = CT_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = CT_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delaware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894736842105263\n",
      "[[13  4]\n",
      " [ 0  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      1.00      0.87        13\n",
      "         1.0       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.88      0.67      0.68        19\n",
      "weighted avg       0.84      0.79      0.75        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = DL_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DL_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "District of Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[421], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m X\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mstateabbr_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstatedesc_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlocationname\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdatasource_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategory_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmeasure_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mdata_value_unit_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata_value_type_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtotalpopulation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgeolocation_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_x\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlow_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh_confidence_limit_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mshort_question_text_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m y \u001b[39m=\u001b[39m DC_checkup[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X,y,random_state\u001b[39m=\u001b[39;49m\u001b[39m78\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m     11\u001b[0m X_scaler \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit(X_train)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = DC_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = DC_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Georgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9548387096774194\n",
      "[[66  6]\n",
      " [ 1 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.95        67\n",
      "         1.0       0.99      0.93      0.96        88\n",
      "\n",
      "    accuracy                           0.95       155\n",
      "   macro avg       0.95      0.96      0.95       155\n",
      "weighted avg       0.96      0.95      0.95       155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = GA_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = GA_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        38\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = HI_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = HI_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idaho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[ 4  0]\n",
      " [ 0 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         4\n",
      "         1.0       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           1.00        52\n",
      "   macro avg       1.00      1.00      1.00        52\n",
      "weighted avg       1.00      1.00      1.00        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ID_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = ID_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880597014925373\n",
      "[[ 19   1]\n",
      " [  3 312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.86      0.90        22\n",
      "         1.0       0.99      1.00      0.99       313\n",
      "\n",
      "    accuracy                           0.99       335\n",
      "   macro avg       0.97      0.93      0.95       335\n",
      "weighted avg       0.99      0.99      0.99       335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IL_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IL_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9365079365079365\n",
      "[[23  2]\n",
      " [ 2 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92        25\n",
      "         1.0       0.95      0.95      0.95        38\n",
      "\n",
      "    accuracy                           0.94        63\n",
      "   macro avg       0.93      0.93      0.93        63\n",
      "weighted avg       0.94      0.94      0.94        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = IN_checkup.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = IN_checkup['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927083333333334\n",
      "[[ 96 173]\n",
      " [299 968]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.24      0.29       395\n",
      "         1.0       0.76      0.85      0.80      1141\n",
      "\n",
      "    accuracy                           0.69      1536\n",
      "   macro avg       0.56      0.55      0.55      1536\n",
      "weighted avg       0.66      0.69      0.67      1536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = checkup_df.copy()\n",
    "X.drop(['stateabbr_x','statedesc_x','locationname','datasource_x','category_x','measure_x','data_value_unit_x',\n",
    "        'data_value_type_x','low_confidence_limit_x','high_confidence_limit_x','totalpopulation_x','geolocation_x','short_question_text_x',\n",
    "        'low_confidence_limit_y','high_confidence_limit_y','short_question_text_y','y'],axis=1, inplace=True)\n",
    "\n",
    "y = checkup_df['y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=78)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "\n",
    "prediction = knn.predict(X_test_scaled)\n",
    "\n",
    "print(accuracy_score(y_test,prediction))\n",
    "print(confusion_matrix(prediction,y_test))\n",
    "print(classification_report(y_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
